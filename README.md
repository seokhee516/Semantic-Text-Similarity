# ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ì¸¡ì •
# 1. í”„ë¡œì íŠ¸ ê°œìš”

<aside>
    
- ë¶€ìŠ¤íŠ¸ìº í”„ AI Techì—ì„œ ê°œìµœí•œ NLP ê¸°ì´ˆëŒ€íšŒ  
- ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ì¸¡ì •: ì˜ë¯¸ ìœ ì‚¬ë„ íŒë³„(Semantic Text Similarity, STS)ì´ë€ ë‘ ë¬¸ì¥ì´ ì˜ë¯¸ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ë¥¼ ìˆ˜ì¹˜í™”í•˜ëŠ” NLP Task  
- ëŒ€íšŒê¸°ê°„: 2022.10.26 ~ 2022.11.03
- ë°ì´í„°ì…‹: í•™ìŠµ ë°ì´í„°ì…‹ 9,324ê°œ, ê²€ì¦ ë°ì´í„°ì…‹ 550ê°œ, í‰ê°€ ë°ì´í„°ëŠ” 1,100ê°œ. í‰ê°€ ë°ì´í„°ì˜ 50%ëŠ” Public ì ìˆ˜ ê³„ì‚°ì— í™œìš©ë˜ì–´ ì‹¤ì‹œê°„ ë¦¬ë”ë³´ë“œì— í‘œê¸°ê°€ ë˜ê³ , ë‚¨ì€ 50%ëŠ” Private ê²°ê³¼ ê³„ì‚°ì— í™œìš©ë˜ì–´ ëŒ€íšŒ ì¢…ë£Œ í›„ í‰ê°€
- í‰ê°€ë°©ë²•: 0ê³¼ 5ì‚¬ì´ì˜ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì˜ˆì¸¡. í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜(Pearson Correlation Coefficient ,PCC) ì§€í‘œ

</aside>

### TimeLine

![Untitled](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/8c31dd14-8481-4733-8767-d13a6afa9076/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221201%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221201T083306Z&X-Amz-Expires=86400&X-Amz-Signature=be620327ae19cc6b55d440dd059d82d532f7a6795c4424638f16735a789e61a9&X-Amz-SignedHeaders=host&response-content-disposition=filename%3D%22Untitled.png%22&x-id=GetObject)

### í˜‘ì—… ë°©ì‹

> **Notion**
> 

Team Notionì— ê° íŒ€ì˜ í˜„í™©ê³¼ ì‹¤í—˜ ê²°ê³¼ë¥¼ ê¸°ë¡ ë° ê³µìœ .

![Untitled (33)](https://user-images.githubusercontent.com/86893209/207260426-837d1777-2d43-4842-a4d4-e9760fd75ed3.png)

> **Git**
> 

![Untitled (31)](https://user-images.githubusercontent.com/86893209/207260463-f6b316bc-2061-4057-8ecb-7aee0750ff9d.png)

![Untitled (32)](https://user-images.githubusercontent.com/86893209/207260493-8da5cdc9-3aec-4f1a-86b7-bc7a64312f4a.png)

master branchì—ì„œ baseline ìˆ˜ì • í›„, íŒ€ì›ì˜ ì´ë¦„ìœ¼ë¡œ ë¶„ê¸°ë¥¼ ë‚˜ëˆ„ì–´ ì‘ì—… ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. Wandb, config íŒŒì¼ ì—°ê²° ë“± ë¶„ì—…í•˜ì—¬ ì‘ì—… í›„, ê°ì branchë¡œ mergeí•˜ì˜€ìŠµë‹ˆë‹¤.

# 2. í”„ë¡œì íŠ¸ íŒ€ êµ¬ì„± ë° ì—­í• 

ğŸ”¬**EDA** : ë‹¨ìµ

> Exploratory Data Analysis, Reference searching
> 

ğŸ—‚ï¸**Data** : ì¬ë•, ì„í¬

> Data Augmentation, searching the pre-trained models
> 

ğŸ§¬**MODEL** : ê±´ìš°, ìš©ì°¬

> to reconstruct the baseline, searching the pre-trained models
> 

# 3. í”„ë¡œì íŠ¸ ìˆ˜í–‰ ì ˆì°¨ ë° ë°©ë²•

![Untitled (41)](https://user-images.githubusercontent.com/86893209/209278066-02d413d5-7f67-4052-9309-0a767e68d349.png)

## 1) íƒìƒ‰ì  ë¶„ì„ ë° ì „ì²˜ë¦¬(EDA) - í•™ìŠµ ë°ì´í„° ì†Œê°œ

![Untitled (40)](https://user-images.githubusercontent.com/86893209/209278059-825e3621-8016-4052-ba8c-2266c67c1252.png)

train.csv

- ë‘ ë¬¸ì¥ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ í”„ë¡œì íŠ¸ì˜ ìµœì¢… ëª©í‘œì´ê³ , ë°ì´í„°ì…‹ì€ train(9,324 rows)/dev(550 rows)/test(1,100 rows) ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì–´, csví˜•íƒœë¡œ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤.
- ê° ë¬¸ì¥ì˜ ì¶œì²˜ëŠ” êµ­ë¯¼ì²­ì› ê²Œì‹œíŒ ì œëª©, ë„¤ì´ë²„ ì˜í™” ê°ì„± ë¶„ì„ ì½”í¼ìŠ¤, ì—…ìŠ¤í…Œì´ì§€ ìŠ¬ë™ ë°ì´í„°ì´ë©°. ê° ë°ì´í„°ë³„ ìœ ì‚¬ë„(Label) ì ìˆ˜ëŠ” ì—¬ëŸ¬ëª…ì˜ ì‚¬ëŒì´ ê³µí†µì˜ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‘ ë¬¸ì¥ê°„ì˜ ì ìˆ˜ë¥¼ í‰ê· ë‚¸ ê°’ì…ë‹ˆë‹¤.

![Untitled (35)](https://user-images.githubusercontent.com/86893209/207260557-0ad7c27d-c969-45da-8ced-07ed945a81ba.png)

train.csv : 9,324 rows

![Untitled (36)](https://user-images.githubusercontent.com/86893209/207260585-e63f91b9-1cb7-4399-9e24-085801d3d11c.png)

dev.csv : 550 rows

- train ë°ì´í„°ì…‹ì˜ Labelë³„ ë°ì´í„° ë¶„í¬ ì‹œê°í™”ë¥¼ í†µí•´, Label 0ìœ¼ë¡œ ì ë¦° ë¶ˆê· í˜• ë°ì´í„°ë¥¼ ê´€ì¸¡í–ˆìŠµë‹ˆë‹¤. ë°˜ë©´, dev.csv ë°ì´í„°ëŠ” ëª¨ë“  labelì˜ ë¶„í¬ê°€ ëŒ€ì²´ë¡œ ê· ì¼í•œ í¸ìœ¼ë¡œ ê´€ì¸¡ë˜ì—ˆìŠµë‹ˆë‹¤.
- trainì˜ ë°ì´í„° ë¶ˆê· í˜•ì„ í•´ì†Œí•˜ê¸° ìœ„í•´, ****label 0ì¸ ë°ì´í„°ë¥¼ ì¤„ì—¬ì„œ ë‹¤ë¥¸ labelê³¼ ë¶„í¬ë¥¼ ë§ì¶”ê±°ë‚˜, label 5ë¥¼ ëŠ˜ë ¤ì„œ ê· í˜•ì„ ë§ì¶”ëŠ” ë°©ë²•ìœ¼ë¡œ ë°ì´í„° í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ í•´ê²°í•˜ê³ ì í–ˆìŠµë‹ˆë‹¤.(Data Augmentation)

## 2) Modeling

### Baseline Code ìˆ˜ì •

- Wandb, Wandb Sweep êµ¬í˜„
- yaml+OmegaConf+Shell í™œìš©í•œ ëª¨ë¸í•™ìŠµ ë° ì‹¤í—˜ê´€ë¦¬ í¸ì˜ì„± ì¦ëŒ€

### ê°€ì¥ ì¢‹ì€ Pre-trained Model ì„ íƒ

- í•œêµ­ì–´ ê¸°ë°˜ RoBERTa, ELECTRA Pre-trained Model ë“¤ì„ ë¹„êµí•´ë³´ì•˜ê³ , snunlp/KR-ELECTRA-discriminatorê°€ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- ì´í›„ snunlp/KR-ELECTRA-discriminator ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ Data Augmentation ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

## 3) Data pre-processing

### Data Augmentation

Baseline Model(klue/roberta-base, Loss: L1, Optimizer: AdamW) ê¸°ì¤€ìœ¼ë¡œ ì•„ë˜ 4ê°€ì§€ ì¦ê°•ê¸°ë²• ë° ìŠ¤ë¬´ë”© ê¸°ë²•ì„ ì ìš©í•˜ì—¬ Data pre-processingì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ì›ë³¸ ë°ì´í„°ì™€ ì¦ê°•ëœ ë°ì´í„°ì˜ ë¹„ìœ¨ì„ ì¡°ì • í•˜ë©´ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì˜€ê³ , ì—¬ëŸ¬ ì¦ê°• ê¸°ë²•ì„ ì¤‘ë³µ ì ìš©í•˜ëŠ” ë“± ìµœì ì˜ ì¡°í•©ì„ ì°¾ì•„ë‚´ê³ ì í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ ì™¸, í•™ìŠµì˜ ì†ë„ë¥¼ ìœ„í•´ learning rateë¥¼ ëŠ˜ë ¤ì£¼ê±°ë‚˜, generalí•œ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ batch sizeë“±ì„ ëŠ˜ë ¤ì£¼ì—ˆìŠµë‹ˆë‹¤.

- **Back TranslationÂ¹â¾**
    - í•œêµ­ì–´ì—ì„œ ì˜ì–´ ë²ˆì—­ í›„, ì˜ì–´ì—ì„œ í•œêµ­ì–´ ì—­ë²ˆì—­
    - ì—­ ë²ˆì—­ ì‹œ ë¶€ì ì ˆí•œ ë²ˆì—­ ê²°ê³¼ì™€ ë°œìƒí•˜ì—¬ ì¼ê´€ëœ ì ìˆ˜ ê¸°ì¤€ì´ ì¤‘ìš”í•œ STS Taskì— ì ì ˆí•˜ì§€ ëª»í•œ ê¸°ë²•ì´ë¼ íŒë‹¨í•˜ì—¬ ì œì™¸í•˜ì˜€ìŠµë‹ˆë‹¤.
- **Copied TranslationÂ²â¾**
    - sentence1ì„ sentence 2ë¡œ ë³µì‚¬í•˜ì—¬, label 5 ë°ì´í„° ìƒì„±
    - Train Dataset ë¶„í¬ ë¶„ì„ ê²°ê³¼ 5 Label ë°ì´í„°ê°€ ì „ì²´ ë°ì´í„°ì…‹ì˜ 1%ì´ê¸°ì— Sentenceê°€ ì„œë¡œ ê°™ì€ ë¬¸ì¥ì„ ì› ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œë§í•˜ì—¬ 5 Label ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.
- **Swap** **Sentence**
    - sentence1ê³¼ sentence 2ì˜ ìˆœì„œë¥¼ ë°”ê¿”ì¤Œ
    - Sentence 1ê³¼ Sentece 2ì˜ Segment Embedding ê°’ì´ ë‹¤ë¥´ê¸°ì— ë³€ê²½ ì‹œ ìœ ì˜ë¯¸í•œ ë°ì´í„° ì¦ê°•ì´ ë  ê²ƒì´ë¼ê³  ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤. ì‹œë„í•œ ë°©ë²• ì¤‘ ê°€ì¥ íš¨ê³¼ê°€ ì¢‹ì•˜ìŠµë‹ˆë‹¤.
- **Reverse TextÂ³â¾**
    - ë¬¸ìë¥¼ ì—­ìˆœìœ¼ë¡œ ìƒì„±
    - ë‹¨ë… ì‚¬ìš©ì‹œ íš¨ê³¼ê°€ ìˆì—ˆê³  ì´ë¥¼ í†µí•´ ìœ ì˜ë¯¸í•œ ë…¸ì´ì¦ˆ ê°’ì„ ìƒì„±í•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ ë¶„ì„í–ˆì§€ë§Œ ì—¬ëŸ¬ ê¸°ë²•ê³¼ í•¨ê»˜ ì‚¬ìš©ì‹œ ì„±ëŠ¥ì´ í•˜ë½í•˜ì—¬ ì œì™¸í•˜ì˜€ìŠµë‹ˆë‹¤.
- **Label Smoothing**
    - label 0 ë°ì´í„° ì œê±°
    - Train Datasetì˜ 50% ì´ìƒì´ 0 Label ì´ê¸°ì— í•´ë‹¹ Labelì„ 50% ì–¸ë” ìƒ˜í”Œë§ í•˜ì˜€ìŠµë‹ˆë‹¤.
    - ì´ë¥¼ Copied Translationê³¼ í•¨ê»˜ ì‚¬ìš©í• ì‹œ íš¨ê³¼ê°€ ì¢‹ì•˜ìŠµë‹ˆë‹¤. ì´ë¥¼ ì› ë¶„í¬ì¸ Positive Skewness ë¶„í¬ì—ì„œ ë¹„êµì  Uniformí•œ ë¶„í¬ë¡œ ë³€ê²½ëœ ê²°ê³¼ë¼ê³  ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤.

ì‹¤í—˜ ê²°ê³¼, learning rate 1e-5, batch size 16ì—ì„œëŠ” **Copied Translation, Reverse Text**, learning rate 2e-5, batch size 32ì—ì„œëŠ” **Swap Sentence, Label Smoothing, Copied Translation**ì´ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## 4) O**ptimization**

### Hyperparameter ì‹¤í—˜ ë° ë¹„êµ

- Loss, Batch Size, Learning rate, Dataì— ë”°ë¥¸ ì‹¤í—˜ ë° ë¹„êµ
    
    
    | Loss | MSE | L1 |  |
    | --- | --- | --- | --- |
    | Batch Size | 16 | 32 |  |
    | Learning rate | 1e-5 | 3e-5 | 5e-5 |
    | Data | Label Smoothing 0, 
    Copied Translation Label 5 | Swap Sentence |  |
    
    | Model | Loss | Learning rate | Batch Size | Val Pearson |
    | --- | --- | --- | --- | --- |
    | RoBERTa Large - Label Smoothing 0, Copied Translation Label 5 | MSE | 7e-6 | 8 | 0.9256 |
    | ELECTRA - Swap Sentence | MSE | 3e-5 | 32 | 0.9287 |
    | ELECTRA - Label Smoothing 0, Copied Translation Label 5, Swap Sentence | MSE | 3e-5 | 16 | 0.9309 |

### snunlp/KR-ELECTRA-discriminator ìµœì í™”

- ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ Pre-trained ëª¨ë¸ì¸ snunlp/KR-ELECTRA-discriminatorì™€ Data Augmentation ì‹¤í—˜ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì  ì¡°í•©ì„ ì‹¤í—˜í•˜ì˜€ìŠµë‹ˆë‹¤.
- ì‹¤í—˜ ê²°ê³¼, **Swap Sentence**ê°€ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ ë² ì´ìŠ¤ë¼ì¸ì— ì‚¬ìš©ë˜ì—ˆë˜ L1 Loss ë³´ë‹¤ **MSE Loss**ê°€ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬, ì´í›„ ì‹¤í—˜ì—ì„œëŠ” **MSE Loss**ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

### RoBERTa Large ìµœì í™”

![Untitled (37)](https://user-images.githubusercontent.com/86893209/207260672-5d7305d9-1ab6-4b69-addf-1ddc5e709eb4.png)

- klue/roberta-largeì˜ ê²½ìš° ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì„œ í•™ìŠµì´ ìˆ˜í–‰ë˜ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ì—¬, **batch sizeì™€ learning rateë¥¼ ì¡°ì •**í•˜ì—¬ ìµœì í™”í•˜ì˜€ìŠµë‹ˆë‹¤. Data Augmentation ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ê°€ì¥ ìœ ì˜ë¯¸ í–ˆë˜ Swap Sentence, Label Smoothing 0 ë° Copied Translation Label 5, Reverse Text 20% ë°ì´í„°ë¥¼ ê°ê° ì‹¤í—˜í•˜ì˜€ìŠµë‹ˆë‹¤.
- ì‹¤í—˜ ê²°ê³¼, **Label Smoothing 0 ë° Copied Translation Label 5** ë°ì´í„°ì˜ **MSE Loss**, **Learning Rate 7e-6, Batch Size 8** ì¡°ê±´ì—ì„œ **Val Pearson 0.9256**ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

### ELECTRA ìµœì í™”

![Untitled (38)](https://user-images.githubusercontent.com/86893209/207260693-86b3c11c-852f-4000-9034-30788a94cf17.png)

- í•œêµ­ì–´ ELECTRA ëª¨ë¸ 3ê°œ(monologg/koelectra-base-v3-discriminator, beomi/KcELECTRA-base, snunlp/KR-ELECTRA-discriminator)ì™€ ë°ì´í„° ì¦ê°•ì—ì„œ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥í–¥ìƒì„ ë³´ì¸ Swap Sentence, Label Smoothing 0, Copied Translation Label 5 ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, Learning Rate, Batch Size ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.
- ì‹¤í—˜ ê²°ê³¼, **snunlp/KR-ELECTRA-discriminator** ëª¨ë¸, **Label Smoothing 0, Copied Translation Label 5, Swap Sentence** ë°ì´í„°ì˜ **Learning Rate 3e-5 Batch Size 16** ì¡°ê±´ì—ì„œ **Val Pearson 0.9309**ìœ¼ë¡œ **ë‹¨ì¼ ëª¨ë¸ ì¤‘ ê°€ì¥ ë†’ì€ ì„±ëŠ¥**ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

## 5) Ensemble

- í‰ê°€ ì§€í‘œì¸ Pearsonì˜ ê²½ìš° ì„ í˜•ì´ê¸°ì— Outlierì— ì·¨ì•½í•œ íŠ¹ì„±ì´ ìˆìŒ. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°€ì¤‘ í‰ê· ì„ ë„ì…, Outlierì˜ ì˜í–¥ë ¥ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤.â´â¾
- ì•™ìƒë¸”ì€ ì†Œí”„íŠ¸ë³´íŒ… ë°©ì‹ì„ ì±„ìš©í•˜ì—¬ ê° ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë”í•´ì£¼ê³  í‰ê· ì„ ë‚´ì£¼ëŠ” ëŒ€ì‹  ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°€ì¤‘ì¹˜ë¡œ ë‘ì–´ì„œ ê°€ì¤‘ í‰ê· ì„ êµ¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì„ softmax ì¸µì— í†µê³¼ì‹œì¼œì„œ í™•ë¥ ë¡œ ë³€í™˜í•œ í›„ ê° ëª¨ë¸ì´ ì¶œë ¥í•œ logit ê°’ê³¼ ê³±í•´ì£¼ì–´ì„œ ì „ë¶€ ë”í•´ì£¼ì—ˆìŠµë‹ˆë‹¤.
- Swap Sentence ê¸°ë²•ì„ ì ìš©í•˜ì—¬  Positive Skewness ë¶„í¬ì¸ ë°ì´í„°ì™€ Copied Translationê³¼ Label Smoothingì„ ì ìš©í•˜ì—¬ Uniform ë¶„í¬ë¥¼ ê°€ì§„ ë°ì´í„°ë¥¼ í•™ìŠµí•œ ëª¨ë¸ì„ ì•™ìƒë¸”í•˜ì—¬ Test Dataset ë¶„í¬ì— ì˜ì¡´ì ì´ì§€ ì•Šìœ¼ë©° Generalí•œ ëª¨ë¸ì„ ì„¤ê³„í•˜ì˜€ìŠµë‹ˆë‹¤.
- ì²˜ìŒì—ëŠ” ì œì¼ ì„±ëŠ¥ì´ ì¢‹ì€ klue/roberta-largeì™€ snunlp/KR-ELECTRA-discriminator 1ê°œì”© ê°€ì ¸ì™€ì„œ ì„±ëŠ¥ì„ 91.24ì—ì„œ 92.25ë¡œ ê°œì„ í–ˆìŠµë‹ˆë‹¤. ê·¸ í›„ ê° ëª¨ë¸ì„ 3ê°œì”© ì•™ì‚¼ë¸”í•œ ëª¨ë¸ë¡œ ì„±ëŠ¥ì„ 92.69ê¹Œì§€ ì˜¬ë ¸ê³ , ëª¨ë¸ ê°„ì˜ ë‚®ì€ ìƒê´€ê´€ê³„ë¥¼ ê°–ê³  ìˆìœ¼ë©´ ì•™ìƒë¸”ì´ íš¨ê³¼ì ì´ë¼ëŠ” ê·¼ê±°ë¥¼ í† ëŒ€ë¡œ ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì•™ìƒë¸”í•œ ê²°ê³¼ ìµœê³  ì„±ëŠ¥ì¸ 92.9ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤
    
    
    | klue/roberta-large ëª¨ë¸ ìµœê³  ì„±ëŠ¥ 1ê°œ | snunlp/KR-ELECTRA-discriminator ìµœê³  ì„±ëŠ¥ 1ê°œ | ì„±ëŠ¥ ê°œì„ : | 0.9124 â†’ 0.9225 |  |  |
    | --- | --- | --- | --- | --- | --- |
    | klue/roberta-large ëª¨ë¸ 3ê°œ | snunlp/KR-ELECTRA-discriminator 3ê°œ | ì„±ëŠ¥ ê°œì„ :  | 0.9225 â†’ 0.9269 |  |  |
    | klue/roberta-large ëª¨ë¸ 3ê°œ | snunlp/KR-ELECTRA-discriminator 3ê°œ | beomi/KcELECTRA-base 1ê°œ | â€¢ monologg/koelectra-base-v3-discriminator 1ê°œ | ì„±ëŠ¥ ê°œì„ : | 0.9269 â†’ 0.9290 |

# 4. í”„ë¡œì íŠ¸ ìˆ˜í–‰ ê²°ê³¼

![Untitled](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5bf59d2a-8e26-4bf8-9ad6-217b2506cffe/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221201%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221201T083349Z&X-Amz-Expires=86400&X-Amz-Signature=bad420e9e339a79dd06db5ee818f494af7ebdee4bad66c5908e8ca04fc1ff245&X-Amz-SignedHeaders=host&response-content-disposition=filename%3D%22Untitled.png%22&x-id=GetObject)

- ìµœì¢… pearson : 0.9368
- 14íŒ€ ì¤‘ public 4ìœ„ private 3ìœ„

# 5. ê²°ë¡ 

### ë°ì´í„°ì˜ íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•˜ëŠ” ê¸°ì´ˆ ëª¨ë¸ ì„ ì • ì´í›„ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ í†µí•´ ìµœì í™” ë° ì•™ìƒë¸” ìˆ˜í–‰

- ë°ì´í„° ë¶„ì„ì„ í†µí•œ ë°ì´í„° í’ˆì§ˆ ê°œì„ (oversampling, data augmentation)
- ë°ì´í„°ì…‹ì— ì í•©í•œ Pretrained Model ì„ ì • ë° ìµœì í™”
- ë‹¤ì–‘í•œ ê²°ê³¼ì— ëŒ€í•œ ì•™ìƒë¸”(Soft Voting)ì„ ìˆ˜í–‰

# 6. Appendix

- Pre-trained Model ì„ íƒ
    
    
    | Model | Epoch (Earlystopping/Max epoch, Best Check point) | Val loss | Val Pearson |
    | --- | --- | --- | --- |
    | klue/roberta-small | 4/100, 4 | 0.6108 | 0.8523 |
    | klue/roberta-base | 4/100, 4 | 0.533 | 0.8916 |
    | jhgan/ko-sroberta-multitask | 3/100, 3 | 0.5149 | 0.8828 |
    | beomi/KcELECTRA-base | 8/20, 4 | 0.4385 | 0.9113 |
    | snunlp/KR-ELECTRA-discriminator | 9/15, 7 | 0.4705 | 0.9242 |
- Data Augmentation
    
    
    | Model | Epoch | Learning rate | Batch Size | Data Augmentation | Val loss | Val Pearson  |
    | --- | --- | --- | --- | --- | --- | --- |
    | klue/roberta-base | 4 | 1e-5 | 16 | Baseline | 0.533 | 0.8916 |
    |  | 9 | 1e-5 | 16 | ì›ë³¸:Back Translation 50% (2:1) | 0.5864 | 0.8655 |
    |  | 8 | 1e-5 | 16 | ì›ë³¸:Back Translation 33% (3:1) | 0.4987 | 0.8958 |
    |  | 16 | 1e-5 | 16 | ì›ë³¸:Back Translation 25% (4:1) | 0.4308 | 0.91 |
    |  | 16 | 1e-5 | 16 | Copied Translation Label 5 50% | 0.4707 | 0.9126 |
    |  | 5 | 1e-5 | 16 | Copied Translation Label 5 20% | 0.5326 | 0.9024 |
    |  | 10 | 1e-5 | 16 | Copied Translation Label 5 10% | 0.5083 | 0.9082 |
    |  | 5 | 1e-5 | 16 | Reverse Text 50% | 0.4957 | 0.8961 |
    |  | 14 | 1e-5 | 16 | Reverse Text 20% | 0.4464 | 0.9169 |
    |  | 19 | 1e-5 | 16 | Reverse Text 10% | 0.4869 | 0.9074 |
    |  | 3 | 1e-5 | 16 | ì›ë³¸:Exchange Sentence : Reverse Text 10% (1:1:0.2) | 0.4695 | 0.908 |
    |  | 4 | 1e-5 | 16 | ì›ë³¸:Exchange Sentence : Reverse Text 20% (1:1:0.4) | 0.4384 | 0.9118 |
    | klue/roberta-base | 4 | 2e-5 | 32 | Baseline | 0.5919 | 0.8616 |
    |  |  | 2e-5 | 32 | Swap Sentence | 0.5013 | 0.8967 |
    |  |  | 2e-5 | 32 | Swap Sentence : Back Translation (2:1) | 0.5008 | 0.8922 |
    |  |  | 2e-5 | 32 | Swap Sentence : Back Translation (1:1) | 0.4978 | 0.8845 |
    |  |  | 2e-5 | 32 | Swap Sentence, Label Smoothing 0 50% | 0.4892 | 0.8986 |
    |  |  | 2e-5 | 32 | Swap Sentence, Label Smoothing 0 25% | 0.4722 | 0.8963 |
    |  |  | 2e-5 | 32 | Swap Sentence, Label Smoothing 0 50%, Copied Translation Label 5 | 0.4801 | 0.8931 |
    |  |  | 2e-5 | 32 | Swap Sentence, Label Smoothing 0 25%, Copied Translation Label 5 | 0.4536 | 0.9123 |
- snunlp/KR-ELECTRA-discriminator ìµœì í™”
    
    
    | Model | Epoch | Loss | Data Augmentation | Val loss | Val Pearson |
    | --- | --- | --- | --- | --- | --- |
    | snunlp/KR-ELECTRA-discriminator | 10 | MSE | Swap Sentence  | 0.3914 | 0.9238 |
    |  | 6 | MSE | Swap Sentence, Label Smoothing 0 50% | 0.4252 | 0.9096 |
    |  | 12 | L1 | Swap Sentence   | 0.5068 | 0.9001 |
    |  | 17 | L1 | Swap Sentence, Label Smoothing 0 50% | 0.4605 | 0.9172 |
    |  | 12 | L1 | Swap Sentence, Label Smoothing 0 50% + Copied Translation Label 5 50% | 0.4484 | 0.9235 |
    |  | 10 | L1 | Swap Sentence, Label Smoothing 0 50%, Reverse Text 20% | 0.4486 | 0.919 |
- RoBERTa Large ìµœì í™”
    
    
    | Model | Epoch  | Learning rate | Batch Size | Data Augmentation | Val loss | Val Pearson |
    | --- | --- | --- | --- | --- | --- | --- |
    | klue/roberta-large | 9 | 1e-6 | 8 | Swap Sentence | 0.4043 | 0.913 |
    |  | 5 | 3e-6 |  | Swap Sentence | 0.4513 | 0.9116 |
    |  | 11 | 5e-6 |  | Swap Sentence | 0.354 | 0.9208 |
    |  | 7 | 7e-6 |  | Swap Sentence | 0.379 | 0.9201 |
    |  | X | 1e-6 |  | Label Smoothing 0, Copied Translation Label 5 | X | X |
    |  | 10 | 3e-6 |  | Label Smoothing 0, Copied Translation Label 5 | 0.3726 | 0.9171 |
    |  | 6 | 5e-6 |  | Label Smoothing 0, Copied Translation Label 5 | 0.3845 | 0.9121 |
    |  | 8 | 7e-6 |  | Label Smoothing 0, Copied Translation Label 5 | 0.3363 | 0.9256 |
    |  | 5 | 5e-6 |  | Copied Translation Label 5 | 0.4841 | 0.9019 |
    |  | 8 | 5e-6 |  | Reverse Text 20% | 0.4631 | 0.91 |

1.  [Data Augmentation using Back-translation for Context-aware Neural Machine Translation](https://aclanthology.org/D19-6504.pdf)

2.  [ì‹ ê²½ë§ ê¸°ê³„ë²ˆì—­ì—ì„œ ìµœì í™”ëœ ë°ì´í„° ì¦ê°•ê¸°ë²• ê³ ì°°](https://koreascience.kr/article/CFKO201930060755841.pdf) - â€œì‹¤í—˜ê²°ê³¼ Back Translationê³¼ Copied Translationì„ í•¨ê»˜ ì ìš©í•˜ì—¬, 4ëŒ€3ì˜ ìƒëŒ€ì  ë¹„ìœ¨ì„ ì ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í–ˆì„ ë•Œ ê°€ì¥ ë†’ì€ BLEU ì ìˆ˜ë¥¼ ë³´ì˜€ë‹¤.â€

3.  [Sequence to Sequence Learning with Neural Networks](https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf) - â€œâ€¦ reversing the order of the words in all source sentences (but not target sentences) improved the LSTMâ€™s performance markedlyâ€

4. [Pearson Coefficient of Correlation Explained](https://towardsdatascience.com/pearson-coefficient-of-correlation-explained-369991d93404) - â€œâ€¦ Pearsonâ€™s correlation coefficient, r, is very sensitive to outliers, which can have a very large effect on the line of best fit and the Pearson correlation coefficient.â€
